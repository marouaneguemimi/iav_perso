{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Conv1D, SpatialDropout1D, Add, Activation, GlobalAveragePooling1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Configuration\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. PRÉPARATION DES DONNÉES EN SÉQUENCES\n",
    "# ==============================================================================\n",
    "print(\"Chargement et structuration des données...\")\n",
    "data = pd.read_csv('data/processed_data.csv')\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Log-transform cible\n",
    "data.loc[data['is_train'] == 1, 'sales'] = np.log1p(data.loc[data['is_train'] == 1, 'sales'])\n",
    "\n",
    "# Tri CRITIQUE : Il faut que les données soient ordonnées temporellement par série\n",
    "data = data.sort_values(['store_nbr', 'family', 'date']).reset_index(drop=True)\n",
    "\n",
    "# Définition des features\n",
    "# On enlève 'date', 'id', 'is_train' et 'sales' (cible) des features d'entrée\n",
    "EXCLUDED = ['id', 'date', 'sales', 'is_train']\n",
    "FEATURES = [c for c in data.columns if c not in EXCLUDED]\n",
    "\n",
    "# Encodage et Scaling (Rapide)\n",
    "print(\"Encodage et Scaling...\")\n",
    "cat_cols = ['store_nbr', 'family', 'city', 'state', 'type', 'cluster', 'month', 'dayofweek', 'is_holiday']\n",
    "for col in cat_cols:\n",
    "    if col in data.columns:\n",
    "        le = LabelEncoder()\n",
    "        data[col] = le.fit_transform(data[col].astype(str))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# On scale uniquement les colonnes numériques (pas les catégories encodées)\n",
    "num_cols = [c for c in FEATURES if c not in cat_cols]\n",
    "data[num_cols] = scaler.fit_transform(data[num_cols])\n",
    "data[FEATURES] = data[FEATURES].fillna(0)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CRÉATION DU CUBE 3D (Séries, Temps, Features)\n",
    "# ==============================================================================\n",
    "print(\"Transformation en Cube 3D...\")\n",
    "\n",
    "# Constantes\n",
    "N_STORES = 54\n",
    "N_FAMILIES = 33\n",
    "N_SERIES = N_STORES * N_FAMILIES # 1782 séries temporelles\n",
    "N_DAYS = len(data) // N_SERIES   # Nombre de jours total\n",
    "N_FEATURES = len(FEATURES)\n",
    "\n",
    "# Vérification d'intégrité\n",
    "assert len(data) % N_SERIES == 0, \"Erreur: Le nombre de lignes n'est pas un multiple parfait de (Stores * Families)\"\n",
    "\n",
    "# RESHAPE MAGIQUE : (1782, Jours, Features)\n",
    "# X_all contient toutes les features pour toutes les séries\n",
    "X_all = data[FEATURES].values.reshape(N_SERIES, N_DAYS, N_FEATURES)\n",
    "# y_all contient toutes les ventes (cibles)\n",
    "y_all = data['sales'].values.reshape(N_SERIES, N_DAYS, 1)\n",
    "\n",
    "# Séparation Train / Test (basée sur le temps)\n",
    "# Le test set Kaggle est les 16 derniers jours\n",
    "HORIZON = 16\n",
    "LOOKBACK = 60 # On regarde 60 jours en arrière pour prédire\n",
    "\n",
    "# Index de séparation\n",
    "train_end_idx = N_DAYS - HORIZON # Fin du training set (exclut le test Kaggle)\n",
    "\n",
    "# Données d'entraînement (Tout sauf les 16 derniers jours qui sont le test Kaggle)\n",
    "X_train_full = X_all[:, :train_end_idx, :]\n",
    "y_train_full = y_all[:, :train_end_idx, :]\n",
    "\n",
    "print(f\"Forme du Cube 3D Train : {X_train_full.shape}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. GÉNÉRATEUR DE SÉQUENCES (SLIDING WINDOW)\n",
    "# ==============================================================================\n",
    "# Pour entraîner, on ne peut pas juste donner le cube. Il faut créer des paires (X, y)\n",
    "# X: Fenêtre de 60 jours\n",
    "# y: Les 16 jours suivants (Stratégie Directe Multi-Output)\n",
    "\n",
    "def create_dataset(X_3d, y_3d, lookback, horizon):\n",
    "    X_seq, y_seq = [], []\n",
    "    \n",
    "    # On itère sur le temps pour créer des fenêtres\n",
    "    # On prend un pas de 16 pour éviter trop de redondance et aller vite (stride)\n",
    "    # Pour plus de précision, mettre stride=1 (mais beaucoup plus lourd en RAM)\n",
    "    n_timesteps = X_3d.shape[1]\n",
    "    \n",
    "    # On s'arrête avant la fin pour avoir la place pour l'horizon\n",
    "    for t in range(lookback, n_timesteps - horizon + 1, 8): # Stride de 8 jours\n",
    "        # Input : t-lookback à t\n",
    "        window_x = X_3d[:, t-lookback:t, :] \n",
    "        # Output : t à t+horizon\n",
    "        window_y = y_3d[:, t:t+horizon, 0]\n",
    "        \n",
    "        # On empile les 1782 séries pour ce pas de temps\n",
    "        X_seq.append(window_x)\n",
    "        y_seq.append(window_y)\n",
    "        \n",
    "    return np.vstack(X_seq), np.vstack(y_seq)\n",
    "\n",
    "print(\"Génération des fenêtres glissantes (Patience...)...\")\n",
    "# On garde les 16 derniers jours du train_full pour la validation\n",
    "val_split_idx = X_train_full.shape[1] - HORIZON\n",
    "\n",
    "X_train_data = X_train_full[:, :val_split_idx, :]\n",
    "y_train_data = y_train_full[:, :val_split_idx, :]\n",
    "\n",
    "X_val_data = X_train_full[:, val_split_idx-LOOKBACK:, :] # On inclut le lookback pour avoir le contexte\n",
    "y_val_data = y_train_full[:, val_split_idx:, :] # Cible de validation (ce sont les 15 derniers jours connus)\n",
    "\n",
    "# Création des datasets numpy\n",
    "X_train, y_train = create_dataset(X_train_data, y_train_data, LOOKBACK, HORIZON)\n",
    "\n",
    "# Pour la validation, on prend juste la dernière fenêtre disponible\n",
    "X_val = X_val_data[:, :LOOKBACK, :]\n",
    "y_val = y_val_data[:, :HORIZON, 0] # (1782, 16)\n",
    "\n",
    "print(f\"Input Train Shape: {X_train.shape}\") # (Samples, 60, Features)\n",
    "print(f\"Output Train Shape: {y_train.shape}\") # (Samples, 16)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. MODÈLE TCN (VRAIE SÉQUENCE)\n",
    "# ==============================================================================\n",
    "def build_tcn_classic(input_shape, output_horizon):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    x = inputs\n",
    "    \n",
    "    # Bloc TCN : Dilated Convolutions\n",
    "    # Dilation 1, 2, 4, 8, 16 permet de voir 1 + 2 + 4 + 8 + 16 = 31 jours en arrière avec peu de couches\n",
    "    for dilation in [1, 2, 4, 8, 16]:\n",
    "        # Residual connection setup\n",
    "        shortcut = x if x.shape[-1] == 64 else Conv1D(64, 1)(x)\n",
    "        \n",
    "        # Conv1D Dilatée\n",
    "        x = Conv1D(filters=64, \n",
    "                   kernel_size=3, \n",
    "                   dilation_rate=dilation, \n",
    "                   padding='causal', # IMPORTANT : Causal pour ne pas voir le futur\n",
    "                   activation='relu')(x)\n",
    "        x = SpatialDropout1D(0.1)(x)\n",
    "        \n",
    "        # Skip connection\n",
    "        x = Add()([x, shortcut])\n",
    "        x = Activation('relu')(x)\n",
    "    \n",
    "    # On ne garde que la dernière étape de temps (GlobalAveragePooling ou Slicing)\n",
    "    # Ici on utilise GlobalAverage pour résumer toute la fenêtre\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    \n",
    "    # Output layer : Prédit 16 jours d'un coup\n",
    "    outputs = Dense(output_horizon, activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "print(\"\\nConstruction du TCN...\")\n",
    "model = build_tcn_classic((LOOKBACK, N_FEATURES), HORIZON)\n",
    "model.summary()\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. ENTRAÎNEMENT\n",
    "# ==============================================================================\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=15,\n",
    "    batch_size=128,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. PRÉDICTION FINALE (SUR LE TEST SET KAGGLE)\n",
    "# ==============================================================================\n",
    "print(\"\\nPrédiction finale...\")\n",
    "\n",
    "# Pour prédire le futur (Test Kaggle), on prend les 60 derniers jours connus de TOUT le dataset\n",
    "last_window = X_all[:, -LOOKBACK:, :] # Shape (1782, 60, Features)\n",
    "\n",
    "# Prédiction (Log scale)\n",
    "preds_log = model.predict(last_window) # Shape (1782, 16)\n",
    "\n",
    "# Inverse Log\n",
    "preds = np.expm1(preds_log)\n",
    "preds[preds < 0] = 0\n",
    "\n",
    "# Remise en forme pour le fichier de soumission\n",
    "# Les prédictions sont de forme (1782 séries, 16 jours)\n",
    "# Il faut les aplatir dans l'ordre : Série 1 (J1..J16), Série 2 (J1..J16)...\n",
    "# ATTENTION : Il faut s'assurer que l'ordre correspond à celui du fichier test.csv\n",
    "# Le fichier test.csv est trié par Date puis Store puis Family ? Non, souvent Store/Family/Date.\n",
    "\n",
    "# On va reconstruire le DataFrame de soumission proprement\n",
    "# On récupère les IDs du test set original\n",
    "test_df = data[data['is_train'] == 0].copy()\n",
    "# On s'assure que test_df est trié exactement comme notre X_all (Store, Family, Date)\n",
    "test_df = test_df.sort_values(['store_nbr', 'family', 'date'])\n",
    "\n",
    "# Aplatissement des prédictions : (1782, 16) -> (1782 * 16,)\n",
    "# .ravel() aplatit ligne par ligne (Série 1 J1, Série 1 J2 ... Série 1 J16, Série 2 J1 ...)\n",
    "# C'est exactement ce qu'on veut si test_df est trié par (Store, Family) puis Date.\n",
    "flat_preds = preds.ravel()\n",
    "\n",
    "test_df['sales'] = flat_preds\n",
    "\n",
    "# On garde juste id et sales, et on remet dans l'ordre des IDs (pour Kaggle)\n",
    "submission = test_df[['id', 'sales']].sort_values('id')\n",
    "submission.to_csv('submission_tcn_classic_3d.csv', index=False)\n",
    "\n",
    "print(\"Terminé ! Fichier 'submission_tcn_classic_3d.csv' généré.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a467fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
