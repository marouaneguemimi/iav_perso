{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1a9362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import gc # Garbage Collector\n",
    "import os\n",
    "import sys\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3b5d2f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cellule à exécuter si on utilise Colab\n",
    "# import kagglehub\n",
    "\n",
    "# # à executer seulement si on utilise Colab\n",
    "# # ne pas oublier que les dotenv ne sont pas sur vos machines c'est sur collab donc à vous de gérer vos variables d'environnement en local\n",
    "# os.environ[\"KAGGLE_USERNAME\"] = \"eminebassoum\"\n",
    "# os.environ[\"KAGGLE_KEY\"] = \"347be8c875407507a6a20e6b693536d0\"\n",
    "# path = kagglehub.competition_download(\"store-sales-time-series-forecasting\")\n",
    "# print(f\"Dataset downloaded to: {path}\")\n",
    "# print(\"Contents of the dataset directory:\")\n",
    "# print(os.listdir(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0470138b",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "afc48351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des données\n",
    "# Check if files exist in 'data' directory, otherwise use kagglehub path\n",
    "data_dir = 'data' if os.path.exists('data') else path\n",
    "\n",
    "train = pd.read_csv(os.path.join(data_dir, 'train.csv'), parse_dates=['date'])\n",
    "test = pd.read_csv(os.path.join(data_dir, 'test.csv'), parse_dates=['date'])\n",
    "stores = pd.read_csv(os.path.join(data_dir, 'stores.csv'))\n",
    "oil = pd.read_csv(os.path.join(data_dir, 'oil.csv'), parse_dates=['date'])\n",
    "transactions = pd.read_csv(os.path.join(data_dir, 'transactions.csv'), parse_dates=['date'])\n",
    "holidays = pd.read_csv(os.path.join(data_dir, 'holidays_events.csv'), parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e023f1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préparation initiale et fusion\n",
    "train['is_train'] = 1\n",
    "test['is_train'] = 0\n",
    "test['sales'] = np.nan\n",
    "data = pd.concat([train, test], sort=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "de4fd2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traitement du prix du pétrole (interpolation)\n",
    "oil['dcoilwtico'] = oil['dcoilwtico'].ffill().bfill()\n",
    "full_dates = pd.date_range(start=data['date'].min(), end=data['date'].max())\n",
    "oil = oil.set_index('date').reindex(full_dates)\n",
    "oil.index.name = 'date'\n",
    "oil['dcoilwtico'] = oil['dcoilwtico'].ffill().bfill()\n",
    "oil = oil.reset_index()\n",
    "\n",
    "# Fusions de base\n",
    "data = data.merge(stores, on='store_nbr', how='left')\n",
    "data = data.merge(oil, on='date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c9632345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention ceci dépend du type de modèle utilisé\n",
    "# Application de la transformation log(1+x) aux ventes dans le jeu d'entraînement\n",
    "# data.loc[data['is_train'] == 1, 'sales'] = np.log1p(data.loc[data['is_train'] == 1, 'sales'])\n",
    "\n",
    "# Note : attention à bien inverser cette transformation lors de la prédiction finale avec np.expm1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8e5ebff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "store_nbr",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "family",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sales",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "onpromotion",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "is_train",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "city",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "state",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cluster",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "dcoilwtico",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "ff1a4d8b-7d7a-4221-b440-af32362beec9",
       "rows": [
        [
         "0",
         "0",
         "2013-01-01 00:00:00",
         "1",
         "AUTOMOTIVE",
         "0.0",
         "0",
         "1",
         "Quito",
         "Pichincha",
         "D",
         "13",
         "93.14"
        ],
        [
         "1",
         "1",
         "2013-01-01 00:00:00",
         "1",
         "BABY CARE",
         "0.0",
         "0",
         "1",
         "Quito",
         "Pichincha",
         "D",
         "13",
         "93.14"
        ],
        [
         "2",
         "2",
         "2013-01-01 00:00:00",
         "1",
         "BEAUTY",
         "0.0",
         "0",
         "1",
         "Quito",
         "Pichincha",
         "D",
         "13",
         "93.14"
        ],
        [
         "3",
         "3",
         "2013-01-01 00:00:00",
         "1",
         "BEVERAGES",
         "0.0",
         "0",
         "1",
         "Quito",
         "Pichincha",
         "D",
         "13",
         "93.14"
        ],
        [
         "4",
         "4",
         "2013-01-01 00:00:00",
         "1",
         "BOOKS",
         "0.0",
         "0",
         "1",
         "Quito",
         "Pichincha",
         "D",
         "13",
         "93.14"
        ]
       ],
       "shape": {
        "columns": 12,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>is_train</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>type</th>\n",
       "      <th>cluster</th>\n",
       "      <th>dcoilwtico</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>93.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>93.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>93.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>93.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>93.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       date  store_nbr      family  sales  onpromotion  is_train   city  \\\n",
       "0   0 2013-01-01          1  AUTOMOTIVE    0.0            0         1  Quito   \n",
       "1   1 2013-01-01          1   BABY CARE    0.0            0         1  Quito   \n",
       "2   2 2013-01-01          1      BEAUTY    0.0            0         1  Quito   \n",
       "3   3 2013-01-01          1   BEVERAGES    0.0            0         1  Quito   \n",
       "4   4 2013-01-01          1       BOOKS    0.0            0         1  Quito   \n",
       "\n",
       "       state type  cluster  dcoilwtico  \n",
       "0  Pichincha    D       13       93.14  \n",
       "1  Pichincha    D       13       93.14  \n",
       "2  Pichincha    D       13       93.14  \n",
       "3  Pichincha    D       13       93.14  \n",
       "4  Pichincha    D       13       93.14  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "15d4cfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date  store_nbr   city      state  is_holiday\n",
      "0 2013-01-01          1  Quito  Pichincha           1\n",
      "1 2013-01-01          1  Quito  Pichincha           1\n",
      "2 2013-01-01          1  Quito  Pichincha           1\n",
      "3 2013-01-01          1  Quito  Pichincha           1\n",
      "4 2013-01-01          1  Quito  Pichincha           1\n"
     ]
    }
   ],
   "source": [
    "# Gestion des jours fériés\n",
    "holidays_events = holidays[holidays['transferred'] == False]\n",
    "\n",
    "# Séparation par portée (National, Regional, Local)\n",
    "holidays_nat = holidays_events[holidays_events['locale'] == 'National'][['date', 'type']].rename(columns={'type': 'national_holiday_type'}).drop_duplicates(subset=['date'])\n",
    "holidays_reg = holidays_events[holidays_events['locale'] == 'Regional'][['date', 'locale_name', 'type']].rename(columns={'locale_name': 'state', 'type': 'regional_holiday_type'})\n",
    "holidays_loc = holidays_events[holidays_events['locale'] == 'Local'][['date', 'locale_name', 'type']].rename(columns={'locale_name': 'city', 'type': 'local_holiday_type'})\n",
    "\n",
    "# Fusions des jours fériés\n",
    "data = data.merge(holidays_nat, on='date', how='left')\n",
    "data = data.merge(holidays_reg, on=['date', 'state'], how='left')\n",
    "data = data.merge(holidays_loc, on=['date', 'city'], how='left')\n",
    "\n",
    "# Création du flag is_holiday et nettoyage\n",
    "data['is_holiday'] = (\n",
    "    data['national_holiday_type'].notna() |\n",
    "    data['regional_holiday_type'].notna() |\n",
    "    data['local_holiday_type'].notna()\n",
    ").astype(int)\n",
    "\n",
    "data.drop(columns=['national_holiday_type', 'regional_holiday_type', 'local_holiday_type'], inplace=True)\n",
    "\n",
    "# %%\n",
    "# Vérification rapide\n",
    "print(data[data['is_holiday'] == 1][['date', 'store_nbr', 'city', 'state', 'is_holiday']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e61f905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Features temporelles basiques\n",
    "data['day'] = data['date'].dt.day\n",
    "data['month'] = data['date'].dt.month\n",
    "data['year'] = data['date'].dt.year\n",
    "data['dayofweek'] = data['date'].dt.dayofweek\n",
    "data['weekofyear'] = data['date'].dt.isocalendar().week.astype(int)\n",
    "data['is_weekend'] = (data['dayofweek'] >= 5).astype(int)\n",
    "data['is_payday'] = ((data['date'].dt.is_month_end) | (data['date'].dt.day == 15)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "de23bad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création des caractéristiques temporelles (ventes et transactions)...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1432"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Intégration des transactions\n",
    "data = data.merge(transactions, how=\"left\", on=[\"date\", \"store_nbr\"])\n",
    "\n",
    "# Imputation des transactions manquantes\n",
    "data.loc[data['transactions'].isnull() & (data['sales'] == 0), 'transactions'] = 0\n",
    "data['transactions'] = data['transactions'].fillna(data.groupby(['store_nbr'])['transactions'].transform('mean'))\n",
    "\n",
    "# Création des features avancées (Lags & Rolling)\n",
    "print(\"Création des caractéristiques temporelles (ventes et transactions)...\")\n",
    "\n",
    "data.sort_values(by=['store_nbr', 'family', 'date'], inplace=True)\n",
    "SHIFT_DAYS = 16\n",
    "\n",
    "# Groupers\n",
    "trans_grouper = data.groupby(['store_nbr', 'family'])['transactions']\n",
    "sales_grouper = data.groupby(['store_nbr', 'family'])['sales']\n",
    "\n",
    "# Séries décalées de base\n",
    "shifted_transactions = trans_grouper.shift(SHIFT_DAYS)\n",
    "base_shifted_sales = sales_grouper.shift(SHIFT_DAYS)\n",
    "\n",
    "# --- Features Transactions ---\n",
    "data[f'transactions_lag_{SHIFT_DAYS}'] = shifted_transactions\n",
    "data['transactions_lag_28'] = trans_grouper.shift(28)\n",
    "\n",
    "data['transactions_roll_mean_7'] = shifted_transactions.rolling(7).mean()\n",
    "data['transactions_roll_mean_28'] = shifted_transactions.rolling(28).mean()\n",
    "data['transactions_roll_std_7'] = shifted_transactions.rolling(7).std()\n",
    "\n",
    "# --- AJOUTS: Leads, Rollings sur Exogènes, Volatilité sur Ventes ---\n",
    "\n",
    "# 3.1. Leads de Promotion (Future Exogène) : Leads 1, 2, 3\n",
    "LEADS_DAYS = [1, 2, 3]\n",
    "promo_grouper = data.groupby(['store_nbr', 'family'])['onpromotion']\n",
    "for lead in LEADS_DAYS:\n",
    "    # Onpromotion peut varier par famille et magasin, on groupe par les deux\n",
    "    data[f'onpromotion_lead_{lead}'] = promo_grouper.shift(-lead).fillna(0)\n",
    "\n",
    "\n",
    "# 3.2. Moyennes Mobiles de 'onpromotion' (Future Exogène)\n",
    "data['onpromotion_roll_mean_7'] = promo_grouper.rolling(window=7, min_periods=1).mean().reset_index(level=[0,1], drop=True)\n",
    "data['onpromotion_roll_mean_28'] = promo_grouper.rolling(window=28, min_periods=1).mean().reset_index(level=[0,1], drop=True)\n",
    "\n",
    "\n",
    "# 3.3. Moyennes Mobiles de 'oil' (Future Exogène)\n",
    "# Oil est le même pour tous, on groupe seulement par date, puis on merge\n",
    "\n",
    "# 1. Calculer les rollings sur la colonne OIL originale pour avoir une série par DATE\n",
    "# Créer une série de dates complète pour l'index\n",
    "full_dates = pd.date_range(start=data['date'].min(), end=data['date'].max())\n",
    "oil_dates = data[['date', 'dcoilwtico']].drop_duplicates(subset=['date']).set_index('date').reindex(full_dates)\n",
    "oil_dates.index.name = 'date'\n",
    "oil_dates['dcoilwtico'] = oil_dates['dcoilwtico'].ffill().bfill() # (Assurez-vous que l'interpolation a été faite avant)\n",
    "\n",
    "# Calculer les rollings sur la série temporelle journalière\n",
    "oil_roll_7 = oil_dates['dcoilwtico'].rolling(window=7, min_periods=1).mean()\n",
    "oil_roll_28 = oil_dates['dcoilwtico'].rolling(window=28, min_periods=1).mean()\n",
    "\n",
    "# 2. Créer une colonne 'date' pour l'alignement sur le DataFrame 'data'\n",
    "data = data.set_index('date')\n",
    "\n",
    "# 3. Utiliser .loc pour l'alignement direct : c'est très rapide et ne duplique pas la mémoire\n",
    "data['oil_roll_mean_7'] = oil_roll_7.astype(np.float32)\n",
    "data['oil_roll_mean_28'] = oil_roll_28.astype(np.float32)\n",
    "\n",
    "# 4. Remettre 'date' comme une colonne\n",
    "data = data.reset_index()\n",
    "\n",
    "# Libérer la mémoire\n",
    "del oil_dates, oil_roll_7, oil_roll_28\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c0abc54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traitement terminé.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Features Ventes ---\n",
    "data[f'sales_lag_{SHIFT_DAYS}'] = base_shifted_sales\n",
    "data['sales_lag_23'] = base_shifted_sales.shift(7)\n",
    "data['sales_lag_30'] = base_shifted_sales.shift(14)\n",
    "data['sales_lag_44'] = base_shifted_sales.shift(28)\n",
    "data['sales_lag_380'] = base_shifted_sales.shift(364)\n",
    "\n",
    "data['sales_roll_7'] = base_shifted_sales.rolling(window=7, min_periods=1).mean()\n",
    "data['sales_roll_14'] = base_shifted_sales.rolling(window=14, min_periods=1).mean()\n",
    "data['sales_roll_28'] = base_shifted_sales.rolling(window=28, min_periods=1).mean()\n",
    "\n",
    "# Ajouter l'écart-type de la moyenne mobile des ventes (volatilité)\n",
    "# base_shifted_sales est un shift de 16 jours (pour éviter le leakage)\n",
    "data['sales_roll_std_7'] = base_shifted_sales.rolling(window=7, min_periods=1).std()\n",
    "data['sales_roll_std_28'] = base_shifted_sales.rolling(window=28, min_periods=1).std()\n",
    "\n",
    "# Nettoyage final des NaN générés par les lags\n",
    "data.fillna(0, inplace=True)\n",
    "print(\"Traitement terminé.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a93df3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout du Feature \"Tremblement de Terre\"\n",
    "\n",
    "# Date du tremblement de terre\n",
    "earthquake_date = pd.to_datetime('2016-04-16')\n",
    "earthquake_window = 21 # Effet prolongé sur 21 jours\n",
    "\n",
    "# Création d'une série temporelle simple pour le jour de l'événement\n",
    "data['earthquake_impulse'] = (data['date'] == earthquake_date).astype(int)\n",
    "\n",
    "# Création de 21 lags de cette impulsion\n",
    "for i in range(1, earthquake_window + 1):\n",
    "    data[f'earthquake_lag_{i}'] = data['earthquake_impulse'].shift(i)\n",
    "\n",
    "# Remplacer les NaN (avant le début de la série) par 0\n",
    "data[data.columns[data.columns.str.startswith('earthquake')]] = \\\n",
    "    data[data.columns[data.columns.str.startswith('earthquake')]].fillna(0.0)\n",
    "\n",
    "# Suppression de la colonne d'impulsion de base si elle n'est pas nécessaire\n",
    "data.drop(columns=['earthquake_impulse'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "945a6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout des Features de Fourier pour la Saisonnalité Annuelle\n",
    "\n",
    "# Créer les colonnes sin/cos pour une période de 365.25 jours (Année)\n",
    "# Ordre = 4 (8 colonnes, suffisant pour une bonne approximation de la saisonnalité annuelle)\n",
    "def create_fourier_features(df, freq, order, prefix):\n",
    "    # df['date'] est déjà en datetime64[ns]\n",
    "    time = (df['date'] - df['date'].min()).dt.days\n",
    "    for k in range(1, order + 1):\n",
    "        df[f'{prefix}_sin_{k}'] = np.sin(2 * np.pi * k * time / freq)\n",
    "        df[f'{prefix}_cos_{k}'] = np.cos(2 * np.pi * k * time / freq)\n",
    "    return df\n",
    "\n",
    "# Application des features de Fourier (Annuel)\n",
    "data = create_fourier_features(data, freq=365.25, order=4, prefix='annual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fda390e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- AJOUT: Scaling des Covariables Numériques (MinMaxScaler) pour le DL ---\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Identification de toutes les colonnes numériques (y compris les lags/rollings/fourier)\n",
    "# Exclure 'date', 'is_train', 'id', et 'sales' (car 'sales' est déjà log-transformé et doit rester séparé)\n",
    "cols_to_exclude = ['date', 'is_train', 'id', 'sales'] \n",
    "\n",
    "# Laisser 'store_nbr' et 'cluster' pour un éventuel Embedding si vous décidez de ne pas les OHE\n",
    "# Sinon, s'assurer que les colonnes OHE ne sont pas incluses (elles sont déjà 0/1)\n",
    "\n",
    "cols_to_scale = [col for col in data.columns \n",
    "                 if data[col].dtype in [np.float64, np.float32, np.int64] \n",
    "                 and col not in cols_to_exclude \n",
    "                 and not col.startswith(('family_', 'city_', 'state_', 'type_', 'cluster_'))]\n",
    "\n",
    "# Application du MinMax Scaling\n",
    "data[cols_to_scale] = scaler.fit_transform(data[cols_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "15bbbb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) One-Hot Encoding pour le DL (Alternative aux Embeddings)\n",
    "categorical_cols = ['family', 'city', 'state', 'type', 'cluster']\n",
    "#data = pd.get_dummies(data, columns=categorical_cols, prefix=categorical_cols)\n",
    "\n",
    "# b) Store_nbr: Le garder comme un entier pour Embeddings (si possible) ou le laisser OHE\n",
    "# Si vous le voulez en OHE, il faut l'ajouter à la liste `categorical_cols` au-dessus.\n",
    "# Sinon, s'assurer qu'il est de type 'int' si votre modèle DL utilise une couche d'Embedding pour lui.\n",
    "# Le laisser ici en int.\n",
    "data['store_nbr'] = data['store_nbr'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4228d131",
   "metadata": {},
   "source": [
    "## LIGHTGBM AISTUDIO BASELINE 0.44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0a16807e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début du script de baseline LightGBM...\n",
      "Préparation finale des données...\n",
      "Séparation des jeux d'entraînement et de test...\n",
      "Création du jeu de validation...\n",
      "Jeu d'entraînement partiel : 2972640 lignes\n",
      "Jeu de validation : 28512 lignes\n",
      "\n",
      "Entraînement du modèle LightGBM avec suivi de la progression...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's rmse: 0.817864\n",
      "[200]\tvalid_0's rmse: 0.607554\n",
      "[300]\tvalid_0's rmse: 0.585502\n",
      "[400]\tvalid_0's rmse: 0.576006\n",
      "[500]\tvalid_0's rmse: 0.570561\n",
      "[600]\tvalid_0's rmse: 0.565272\n",
      "[700]\tvalid_0's rmse: 0.560235\n",
      "[800]\tvalid_0's rmse: 0.558654\n",
      "[900]\tvalid_0's rmse: 0.557284\n",
      "[1000]\tvalid_0's rmse: 0.5545\n",
      "[1100]\tvalid_0's rmse: 0.552796\n",
      "[1200]\tvalid_0's rmse: 0.552145\n",
      "[1300]\tvalid_0's rmse: 0.551086\n",
      "[1400]\tvalid_0's rmse: 0.549513\n",
      "[1500]\tvalid_0's rmse: 0.548373\n",
      "[1600]\tvalid_0's rmse: 0.54536\n",
      "[1700]\tvalid_0's rmse: 0.541268\n",
      "[1800]\tvalid_0's rmse: 0.536977\n",
      "[1900]\tvalid_0's rmse: 0.53192\n",
      "[2000]\tvalid_0's rmse: 0.523008\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\tvalid_0's rmse: 0.523008\n",
      "\n",
      "Génération des prédictions sur le jeu de test...\n",
      "Meilleure itération trouvée : 2000\n",
      "\n",
      "Fichier 'submission_baseline_lgbm_v2.csv' créé avec succès !\n",
      "           id     sales\n",
      "1684  3000888  4.497196\n",
      "1685  3002670  4.394156\n",
      "1686  3004452  4.604638\n",
      "1687  3006234  5.376237\n",
      "1688  3008016  3.563222\n"
     ]
    }
   ],
   "source": [
    "print(\"Début du script de baseline LightGBM...\")\n",
    "\n",
    "# ==============================================================================\n",
    "# ÉTAPE 1: PRÉPARATION FINALE POUR LE MODÈLE\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"Préparation finale des données...\")\n",
    "\n",
    "# !! CORRECTION : On ne supprime pas la colonne 'date' tout de suite !!\n",
    "if 'transactions' in data.columns:\n",
    "    data.drop(columns=['transactions'], inplace=True)\n",
    "\n",
    "# Conversion des colonnes catégorielles\n",
    "categorical_features = [\n",
    "    \"store_nbr\", \"family\", \"city\", \"state\", \"type\", \"cluster\",\n",
    "    \"is_holiday\", \"dayofweek\", \"month\"\n",
    "]\n",
    "for col in categorical_features:\n",
    "    if col in data.columns:\n",
    "        data[col] = data[col].astype('category')\n",
    "\n",
    "# ==============================================================================\n",
    "# ÉTAPE 2: SÉPARATION DES DONNÉES EN ENTRAÎNEMENT ET TEST\n",
    "# ==============================================================================\n",
    "print(\"Séparation des jeux d'entraînement et de test...\")\n",
    "train_df = data[data['is_train'] == 1].copy()\n",
    "test_df = data[data['is_train'] == 0].copy()\n",
    "\n",
    "# ==============================================================================\n",
    "# ÉTAPE 3 BIS: CRÉATION D'UN JEU DE VALIDATION (MÉTHODE CORRIGÉE)\n",
    "# ==============================================================================\n",
    "print(\"Création du jeu de validation...\")\n",
    "\n",
    "# On utilise la colonne 'date' QUI EXISTE DÉJÀ DANS train_df\n",
    "last_train_date = train_df['date'].max()\n",
    "validation_start_date = last_train_date - pd.DateOffset(days=15)\n",
    "\n",
    "# On crée le masque DIRECTEMENT à partir de train_df. Les longueurs correspondront parfaitement.\n",
    "valid_indices = train_df[train_df['date'] >= validation_start_date].index\n",
    "train_indices = train_df[train_df['date'] < validation_start_date].index\n",
    "\n",
    "# Créer les jeux de données partiels\n",
    "train_part_df = train_df.loc[train_indices]\n",
    "valid_df = train_df.loc[valid_indices]\n",
    "\n",
    "print(f\"Jeu d'entraînement partiel : {train_part_df.shape[0]} lignes\")\n",
    "print(f\"Jeu de validation : {valid_df.shape[0]} lignes\")\n",
    "\n",
    "# ==============================================================================\n",
    "# ÉTAPE 4: DÉFINITION DES FEATURES (X) ET DE LA CIBLE (y)\n",
    "# ==============================================================================\n",
    "\n",
    "# La cible est la colonne 'sales'\n",
    "y_train_part = np.log1p(train_part_df['sales'])\n",
    "y_valid = np.log1p(valid_df['sales'])\n",
    "\n",
    "# Les features sont toutes les autres colonnes utiles\n",
    "# !! CORRECTION : On supprime 'date' et les autres colonnes inutiles ICI !!\n",
    "features = [col for col in train_df.columns if col not in ['id', 'sales', 'is_train', 'date']]\n",
    "\n",
    "X_train_part = train_part_df[features]\n",
    "X_valid = valid_df[features]\n",
    "X_test = test_df[features] # On utilise la même liste de features pour le test\n",
    "\n",
    "# Garder les IDs pour la soumission\n",
    "test_ids = test_df['id'].copy()\n",
    "\n",
    "# Libérer de la mémoire\n",
    "del train_df, test_df, data, train_part_df, valid_df\n",
    "gc.collect()\n",
    "\n",
    "# ==============================================================================\n",
    "# ÉTAPE 5: ENTRAÎNEMENT DU MODÈLE LIGHTGBM (inchangé)\n",
    "# ==============================================================================\n",
    "print(\"\\nEntraînement du modèle LightGBM avec suivi de la progression...\")\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'regression_l1', 'metric': 'rmse', 'n_estimators': 2000,\n",
    "    'learning_rate': 0.02, 'feature_fraction': 0.8, 'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1, 'lambda_l1': 0.1, 'lambda_l2': 0.1, 'num_leaves': 31,\n",
    "    'verbose': -1, 'n_jobs': -1, 'seed': 42, 'boosting_type': 'gbdt',\n",
    "}\n",
    "\n",
    "model = lgb.LGBMRegressor(**lgb_params)\n",
    "\n",
    "model.fit(X_train_part, y_train_part,\n",
    "          eval_set=[(X_valid, y_valid)],\n",
    "          eval_metric='rmse',\n",
    "          callbacks=[\n",
    "              lgb.log_evaluation(period=100),\n",
    "              lgb.early_stopping(stopping_rounds=100)\n",
    "          ],\n",
    "          categorical_feature=[col for col in categorical_features if col in features])\n",
    "\n",
    "# ==============================================================================\n",
    "# ÉTAPE 6: PRÉDICTION ET CRÉATION DU FICHIER DE SOUMISSION (inchangé)\n",
    "# ==============================================================================\n",
    "print(\"\\nGénération des prédictions sur le jeu de test...\")\n",
    "\n",
    "best_iteration = model.best_iteration_ if model.best_iteration_ else lgb_params['n_estimators']\n",
    "print(f\"Meilleure itération trouvée : {best_iteration}\")\n",
    "\n",
    "predictions_log = model.predict(X_test, num_iteration=best_iteration)\n",
    "predictions = np.expm1(predictions_log)\n",
    "predictions[predictions < 0] = 0\n",
    "\n",
    "submission_df = pd.DataFrame({'id': test_ids, 'sales': predictions})\n",
    "submission_df.to_csv('submission_baseline_lgbm_v2.csv', index=False)\n",
    "\n",
    "print(\"\\nFichier 'submission_baseline_lgbm_v2.csv' créé avec succès !\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
