{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da735715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc # Garbage Collector\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# --- IMPORTS DEEP LEARNING ---\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, concatenate, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e04b3ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3029664 entries, 0 to 3029663\n",
      "Data columns (total 34 columns):\n",
      " #   Column                     Dtype  \n",
      "---  ------                     -----  \n",
      " 0   id                         int64  \n",
      " 1   date                       object \n",
      " 2   store_nbr                  int64  \n",
      " 3   family                     object \n",
      " 4   sales                      float64\n",
      " 5   onpromotion                int64  \n",
      " 6   is_train                   int64  \n",
      " 7   city                       object \n",
      " 8   state                      object \n",
      " 9   type                       object \n",
      " 10  cluster                    int64  \n",
      " 11  dcoilwtico                 float64\n",
      " 12  is_holiday                 int64  \n",
      " 13  day                        int64  \n",
      " 14  month                      int64  \n",
      " 15  year                       int64  \n",
      " 16  dayofweek                  int64  \n",
      " 17  weekofyear                 int64  \n",
      " 18  is_weekend                 int64  \n",
      " 19  is_payday                  int64  \n",
      " 20  transactions               float64\n",
      " 21  transactions_lag_16        float64\n",
      " 22  transactions_lag_28        float64\n",
      " 23  transactions_roll_mean_7   float64\n",
      " 24  transactions_roll_mean_28  float64\n",
      " 25  transactions_roll_std_7    float64\n",
      " 26  sales_lag_16               float64\n",
      " 27  sales_lag_23               float64\n",
      " 28  sales_lag_30               float64\n",
      " 29  sales_lag_44               float64\n",
      " 30  sales_lag_380              float64\n",
      " 31  sales_roll_7               float64\n",
      " 32  sales_roll_14              float64\n",
      " 33  sales_roll_28              float64\n",
      "dtypes: float64(16), int64(13), object(5)\n",
      "memory usage: 785.9+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/processed_data.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "daa6f7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LOG-TRANSFORMATION DE LA CIBLE ---\n",
    "\n",
    "# On applique la transformation seulement sur les données d'entraînement (is_train == 1)\n",
    "# car la colonne 'sales' du jeu de test contient des np.nan.\n",
    "\n",
    "data.loc[data['is_train'] == 1, 'sales'] = np.log1p(data.loc[data['is_train'] == 1, 'sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5ad4b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Préparation finale des données pour MLP...\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ÉTAPE 1: PRÉPARATION FINALE POUR LE MODÈLE DEEP LEARNING\n",
    "# ==============================================================================\n",
    "print(\"Préparation finale des données pour MLP...\")\n",
    "\n",
    "# 2. Nettoyage des noms de colonnes (pour LightGBM, mais bonne pratique aussi pour Keras)\n",
    "def clean_feature_names(df):\n",
    "    new_cols = []\n",
    "    for col in df.columns:\n",
    "        col = re.sub(r'[\\[\\]<>]', '', col)\n",
    "        col = re.sub(r'\\s+', '_', col)\n",
    "        col = re.sub(r'[,()\"-]', '', col)\n",
    "        new_cols.append(col)\n",
    "    df.columns = new_cols\n",
    "    return df\n",
    "data = clean_feature_names(data)\n",
    "\n",
    "# 3. Supprimer la colonne transactions originale qui a servi à créer les lags\n",
    "if 'transactions' in data.columns:\n",
    "    data.drop(columns=['transactions'], inplace=True)\n",
    "\n",
    "# Définition des features\n",
    "FEATURES = [col for col in data.columns if col not in ['id', 'sales', 'is_train', 'date']]\n",
    "\n",
    "# Séparation des features en types pour le modèle DL\n",
    "CATEGORICAL_FEATURES_OHE = ['is_holiday', 'dayofweek', 'is_weekend', 'is_payday'] # Simple OHE (ou binaire)\n",
    "CATEGORICAL_FEATURES_EMBEDDING = ['store_nbr', 'family', 'city', 'state', 'type', 'cluster', 'month', 'weekofyear'] # Mieux adapté aux Embeddings\n",
    "NUMERICAL_FEATURES = [col for col in FEATURES if col not in CATEGORICAL_FEATURES_OHE and col not in CATEGORICAL_FEATURES_EMBEDDING]\n",
    "\n",
    "# --- Opérations de Prétraitement Finales sur Dataframe ---\n",
    "\n",
    "# 4. Encodage Label (Obligatoire pour les Embeddings)\n",
    "# Les colonnes d'Embedding DOIVENT commencer à 0.\n",
    "label_encoders = {}\n",
    "for col in CATEGORICAL_FEATURES_EMBEDDING:\n",
    "    if col in data.columns:\n",
    "        le = LabelEncoder()\n",
    "        # Le fit sur l'ensemble complet (train + test) pour éviter les clés inconnues\n",
    "        data[col] = le.fit_transform(data[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# 5. Normalisation des Features Numériques (MinMaxScaler)\n",
    "scaler = MinMaxScaler()\n",
    "data[NUMERICAL_FEATURES] = scaler.fit_transform(data[NUMERICAL_FEATURES])\n",
    "\n",
    "# 6. One-Hot Encoding pour les petites catégories\n",
    "data = pd.get_dummies(data, columns=CATEGORICAL_FEATURES_OHE, drop_first=True)\n",
    "\n",
    "# Mise à jour des FEATURES (si de nouvelles colonnes OHE ont été créées)\n",
    "FEATURES = [col for col in data.columns if col not in ['id', 'sales', 'is_train', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5a75205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Séparation des jeux d'entraînement et de test...\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ÉTAPE 2: SÉPARATION DES DONNÉES EN ENTRAÎNEMENT ET TEST\n",
    "# ==============================================================================\n",
    "print(\"Séparation des jeux d'entraînement et de test...\")\n",
    "train_df = data[data['is_train'] == 1].copy()\n",
    "test_df = data[data['is_train'] == 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c9fc60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant conversion, dtype date: object\n",
      "Après conversion, dtype date: datetime64[ns]\n",
      "Création du jeu de validation...\n",
      "Jeu d'entraînement partiel : 2972640 lignes\n",
      "Jeu de validation : 28512 lignes\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ÉTAPE 3 BIS: CRÉATION D'UN JEU DE VALIDATION\n",
    "# ==============================================================================\n",
    "print(\"Avant conversion, dtype date:\", train_df['date'].dtype)\n",
    "\n",
    "# Forcer 'date' en datetime\n",
    "train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "\n",
    "print(\"Après conversion, dtype date:\", train_df['date'].dtype)\n",
    "\n",
    "print(\"Création du jeu de validation...\")\n",
    "\n",
    "# On utilise la colonne 'date' QUI EXISTE DÉJÀ DANS train_df\n",
    "last_train_date = train_df['date'].max()\n",
    "validation_start_date = last_train_date - pd.DateOffset(days=15)\n",
    "\n",
    "# On crée le masque DIRECTEMENT à partir de train_df. Les longueurs correspondront parfaitement.\n",
    "valid_indices = train_df[train_df['date'] >= validation_start_date].index\n",
    "train_indices = train_df[train_df['date'] < validation_start_date].index\n",
    "\n",
    "# Créer les jeux de données partiels\n",
    "train_part_df = train_df.loc[train_indices]\n",
    "valid_df = train_df.loc[valid_indices]\n",
    "\n",
    "print(f\"Jeu d'entraînement partiel : {train_part_df.shape[0]} lignes\")\n",
    "print(f\"Jeu de validation : {valid_df.shape[0]} lignes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "803a2d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7296"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ÉTAPE 4: DÉFINITION DES FEATURES (X) ET DE LA CIBLE (y)\n",
    "# ==============================================================================\n",
    "\n",
    "# Séparation des jeux de données après le nettoyage et l'encodage\n",
    "train_df = data[data['is_train'] == 1].copy()\n",
    "test_df = data[data['is_train'] == 0].copy()\n",
    "\n",
    "# 1. Séparation Train Part/Validation\n",
    "train_part_df = train_df.loc[train_indices].copy()\n",
    "valid_df = train_df.loc[valid_indices].copy()\n",
    "\n",
    "# Cible y (déjà log-transformée dans la nouvelle étape 1)\n",
    "y_train_part = train_part_df['sales'].values\n",
    "y_valid = valid_df['sales'].values\n",
    "\n",
    "# Features X\n",
    "X_train_part_num = train_part_df[NUMERICAL_FEATURES].values\n",
    "X_valid_num = valid_df[NUMERICAL_FEATURES].values\n",
    "X_test_num = test_df[NUMERICAL_FEATURES].values\n",
    "\n",
    "# Features OHE/Embedding (Attention à l'ordre des colonnes, Keras est strict)\n",
    "X_train_part_cat = train_part_df[[col for col in FEATURES if col not in NUMERICAL_FEATURES]].values\n",
    "X_valid_cat = valid_df[[col for col in FEATURES if col not in NUMERICAL_FEATURES]].values\n",
    "X_test_cat = test_df[[col for col in FEATURES if col not in NUMERICAL_FEATURES]].values\n",
    "\n",
    "test_ids = test_df['id'].copy()\n",
    "\n",
    "# Libérer de la mémoire\n",
    "del train_df, test_df, data, train_part_df, valid_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b06c1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_part_num dtypes: float64\n",
      "X_train_part_cat dtypes: object\n",
      "y_train_part dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Debug rapide des dtypes\n",
    "print(\"X_train_part_num dtypes:\", X_train_part_num.dtype)\n",
    "print(\"X_train_part_cat dtypes:\", X_train_part_cat.dtype)\n",
    "print(\"y_train_part dtype:\", y_train_part.dtype)\n",
    "\n",
    "# Conversion en types compatibles Keras\n",
    "X_train_part_num = X_train_part_num.astype(\"float32\")\n",
    "X_valid_num = X_valid_num.astype(\"float32\")\n",
    "X_test_num = X_test_num.astype(\"float32\")\n",
    "\n",
    "X_train_part_cat = X_train_part_cat.astype(\"int32\")\n",
    "X_valid_cat = X_valid_cat.astype(\"int32\")\n",
    "X_test_cat = X_test_cat.astype(\"int32\")\n",
    "\n",
    "y_train_part = y_train_part.astype(\"float32\")\n",
    "y_valid = y_valid.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "af4671d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entraînement du Modèle MLP Global...\n",
      "Epoch 1/10\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 49ms/step - loss: 0.8300 - root_mean_squared_error: 0.9111 - val_loss: 0.3387 - val_root_mean_squared_error: 0.5820\n",
      "Epoch 2/10\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 47ms/step - loss: 0.2997 - root_mean_squared_error: 0.5474 - val_loss: 0.3061 - val_root_mean_squared_error: 0.5533\n",
      "Epoch 3/10\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 48ms/step - loss: 0.2469 - root_mean_squared_error: 0.4969 - val_loss: 0.3099 - val_root_mean_squared_error: 0.5567\n",
      "Epoch 4/10\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 47ms/step - loss: 0.2227 - root_mean_squared_error: 0.4719 - val_loss: 0.3092 - val_root_mean_squared_error: 0.5560\n",
      "Epoch 5/10\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 47ms/step - loss: 0.2095 - root_mean_squared_error: 0.4577 - val_loss: 0.3092 - val_root_mean_squared_error: 0.5561\n",
      "Epoch 6/10\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 47ms/step - loss: 0.2006 - root_mean_squared_error: 0.4479 - val_loss: 0.2835 - val_root_mean_squared_error: 0.5324\n",
      "Epoch 7/10\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 47ms/step - loss: 0.1957 - root_mean_squared_error: 0.4424 - val_loss: 0.2904 - val_root_mean_squared_error: 0.5389\n",
      "Epoch 8/10\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 50ms/step - loss: 0.1903 - root_mean_squared_error: 0.4362 - val_loss: 0.2864 - val_root_mean_squared_error: 0.5352\n",
      "Epoch 9/10\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 47ms/step - loss: 0.1860 - root_mean_squared_error: 0.4313 - val_loss: 0.2892 - val_root_mean_squared_error: 0.5377\n",
      "Epoch 10/10\n",
      "\u001b[1m726/726\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 47ms/step - loss: 0.1824 - root_mean_squared_error: 0.4271 - val_loss: 0.2935 - val_root_mean_squared_error: 0.5417\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ÉTAPE 5: ENTRAÎNEMENT DU MODÈLE MLP (Deep Learning)\n",
    "# ==============================================================================\n",
    "print(\"\\nEntraînement du Modèle MLP Global...\")\n",
    "\n",
    "# --- 5.1: Définition de l'Architecture ---\n",
    "\n",
    "def create_mlp_model(numerical_features, embedding_features, categorical_features_ohe, label_encoders):\n",
    "    # Dictionnaire pour stocker les inputs Keras (une par feature)\n",
    "    inputs = {}\n",
    "    \n",
    "    # 1. Inputs pour les Embeddings (features catégorielles avec bcp de classes)\n",
    "    embedding_outputs = []\n",
    "    for col in embedding_features:\n",
    "        # Input: Entiers (Label Encoded)\n",
    "        vocab_size = len(label_encoders[col].classes_)\n",
    "        embed_dim = max(1, vocab_size // 4) # Taille de l'embedding: sqrt(vocab_size) ou vocab_size / 4\n",
    "        \n",
    "        input_layer = Input(shape=(1,), name=f'input_{col}')\n",
    "        inputs[col] = input_layer\n",
    "        \n",
    "        # Couche d'Embedding\n",
    "        embedding = Embedding(input_dim=vocab_size, output_dim=embed_dim, name=f'embedding_{col}')(input_layer)\n",
    "        flatten = Flatten()(embedding)\n",
    "        embedding_outputs.append(flatten)\n",
    "        \n",
    "    # 2. Inputs pour les features OHE (déjà gérées par pd.get_dummies) et Numériques\n",
    "    \n",
    "    # Keras ne prend qu'un seul grand input pour les features numériques/OHE\n",
    "    input_numeric_ohe = Input(shape=(X_train_part_num.shape[1] + (X_train_part_cat.shape[1] - len(embedding_features)),), \n",
    "                          name='input_num_ohe')\n",
    "    inputs['num_ohe'] = input_numeric_ohe\n",
    "    \n",
    "    # 3. Concatenation\n",
    "    if embedding_outputs:\n",
    "        # Concaténer les Embeddings avec les features numériques/OHE\n",
    "        all_features = concatenate(embedding_outputs + [input_numeric_ohe])\n",
    "    else:\n",
    "        all_features = input_numeric_ohe\n",
    "        \n",
    "    # 4. Couches Dense du MLP\n",
    "    x = Dense(1024, activation='relu')(all_features)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Output: 1 neurone pour la vente (régression)\n",
    "    output_layer = Dense(1, activation='linear', name='output_sales')(x)\n",
    "    \n",
    "    # Construction et compilation du modèle\n",
    "    # Le modèle prend toutes les couches d'entrée définies\n",
    "    model = Model(inputs=list(inputs.values()), outputs=output_layer)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "# --- 5.2: Création et Entraînement ---\n",
    "\n",
    "# Préparer les inputs pour Keras (un input par embedding + l'input num/ohe)\n",
    "def prepare_keras_inputs(X_cat, X_num, embedding_features):\n",
    "    # X_cat = [embeddings | autres cat/OHE]\n",
    "    \n",
    "    X_embed = X_cat[:, :len(embedding_features)]\n",
    "    X_ohe = X_cat[:, len(embedding_features):]\n",
    "    \n",
    "    keras_inputs = []\n",
    "    \n",
    "    # 1. Inputs d'Embedding (séparés, 1 colonne à la fois)\n",
    "    for i in range(len(embedding_features)):\n",
    "        # Keras veut un array de shape (N, 1) pour l'input d'Embedding\n",
    "        keras_inputs.append(X_embed[:, i].reshape(-1, 1))\n",
    "        \n",
    "    # 2. Input Numérique/OHE (fusionné)\n",
    "    X_num_ohe = np.hstack([X_num, X_ohe])\n",
    "    keras_inputs.append(X_num_ohe)\n",
    "    \n",
    "    return keras_inputs\n",
    "\n",
    "# Création du modèle\n",
    "mlp_model = create_mlp_model(NUMERICAL_FEATURES, CATEGORICAL_FEATURES_EMBEDDING, CATEGORICAL_FEATURES_OHE, label_encoders)\n",
    "# mlp_model.summary() # Décommenter pour voir le résumé du modèle\n",
    "\n",
    "# Préparation des inputs\n",
    "X_train_keras = prepare_keras_inputs(X_train_part_cat, X_train_part_num, CATEGORICAL_FEATURES_EMBEDDING)\n",
    "X_valid_keras = prepare_keras_inputs(X_valid_cat, X_valid_num, CATEGORICAL_FEATURES_EMBEDDING)\n",
    "\n",
    "# Entraînement\n",
    "history = mlp_model.fit(\n",
    "    X_train_keras, y_train_part,\n",
    "    validation_data=(X_valid_keras, y_valid),\n",
    "    epochs=10, # Commence avec 50, ajuster en fonction du temps et de la performance\n",
    "    batch_size=4096, # Taille de lot élevée pour accélérer le training\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a53f89a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Génération des prédictions sur le jeu de test...\n",
      "\u001b[1m891/891\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "\n",
      "Fichier 'submission_mlp_deeplearning.csv' créé avec succès !\n",
      "           id     sales\n",
      "1684  3000888  3.550138\n",
      "1685  3002670  3.384698\n",
      "1686  3004452  3.664725\n",
      "1687  3006234  3.616055\n",
      "1688  3008016  1.955912\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ÉTAPE 6: PRÉDICTION ET CRÉATION DU FICHIER DE SOUMISSION\n",
    "# ==============================================================================\n",
    "print(\"\\nGénération des prédictions sur le jeu de test...\")\n",
    "\n",
    "# Préparation de l'input de test\n",
    "X_test_keras = prepare_keras_inputs(X_test_cat, X_test_num, CATEGORICAL_FEATURES_EMBEDDING)\n",
    "\n",
    "# Prédiction (sortie log-transformée)\n",
    "predictions_log = mlp_model.predict(X_test_keras).flatten()\n",
    "\n",
    "# Inverse de la Log Transformation (np.expm1)\n",
    "predictions = np.expm1(predictions_log)\n",
    "predictions[predictions < 0] = 0\n",
    "\n",
    "submission_df = pd.DataFrame({'id': test_ids, 'sales': predictions})\n",
    "submission_df.to_csv('submission_mlp_deeplearning.csv', index=False)\n",
    "\n",
    "print(\"\\nFichier 'submission_mlp_deeplearning.csv' créé avec succès !\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
