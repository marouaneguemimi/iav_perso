{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6a67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Embedding, Concatenate, Dropout, BatchNormalization, Add, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "# Configuration\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CHARGEMENT ET PRÉPARATION\n",
    "# ==============================================================================\n",
    "print(\"Chargement des données...\")\n",
    "data = pd.read_csv('data/processed_data.csv')\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Log-transform cible (Critique)\n",
    "data.loc[data['is_train'] == 1, 'sales'] = np.log1p(data.loc[data['is_train'] == 1, 'sales'])\n",
    "\n",
    "if 'transactions' in data.columns:\n",
    "    data.drop(columns=['transactions'], inplace=True)\n",
    "\n",
    "# Définition des colonnes\n",
    "ALL_COLS = [c for c in data.columns if c not in ['id', 'sales', 'is_train', 'date']]\n",
    "# On retire 'family' des features car on va boucler dessus !\n",
    "CAT_COLS = ['store_nbr', 'city', 'state', 'type', 'cluster', 'month', 'dayofweek'] \n",
    "NUM_COLS = [c for c in ALL_COLS if c not in CAT_COLS and c != 'family']\n",
    "\n",
    "print(f\"Features numériques : {len(NUM_COLS)}\")\n",
    "print(f\"Features catégorielles : {len(CAT_COLS)}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. PREPROCESSING GLOBAL\n",
    "# ==============================================================================\n",
    "print(\"Preprocessing...\")\n",
    "\n",
    "# Encodage des catégories\n",
    "label_encoders = {}\n",
    "for col in CAT_COLS:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = data[col].astype(str)\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Scaling des numériques\n",
    "scaler = StandardScaler()\n",
    "data[NUM_COLS] = scaler.fit_transform(data[NUM_COLS])\n",
    "data[NUM_COLS] = data[NUM_COLS].fillna(0)\n",
    "\n",
    "# Liste des familles pour la boucle\n",
    "FAMILIES = data['family'].unique()\n",
    "print(f\"Nombre de modèles à entraîner : {len(FAMILIES)}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. DÉFINITION DU MODÈLE RESNET (Fonction)\n",
    "# ==============================================================================\n",
    "def build_resnet_model(num_features, cat_cols_info, label_encoders):\n",
    "    inputs = []\n",
    "    embeddings = []\n",
    "    \n",
    "    # Embeddings\n",
    "    for col in cat_cols_info:\n",
    "        vocab_size = len(label_encoders[col].classes_) + 1\n",
    "        embed_dim = min(50, (vocab_size + 1) // 2)\n",
    "        inp = Input(shape=(1,), name=f'input_{col}')\n",
    "        inputs.append(inp)\n",
    "        emb = Embedding(vocab_size, embed_dim)(inp)\n",
    "        emb = Flatten()(emb)\n",
    "        embeddings.append(emb)\n",
    "    \n",
    "    # Numériques\n",
    "    input_num = Input(shape=(num_features,), name='input_numeric')\n",
    "    inputs.append(input_num)\n",
    "    \n",
    "    # Fusion\n",
    "    x = Concatenate()(embeddings + [input_num])\n",
    "    \n",
    "    # Projection\n",
    "    x = Dense(128)(x) # Un peu plus petit car moins de données par famille\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    # Bloc Résiduel 1\n",
    "    shortcut = x\n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(128)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Add()([x, shortcut]) # Skip connection\n",
    "    \n",
    "    # Bloc Résiduel 2\n",
    "    shortcut = x\n",
    "    x = Dense(64)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(64)(x) # Projection shortcut si dim change\n",
    "    shortcut = Dense(64)(shortcut)\n",
    "    x = Add()([x, shortcut])\n",
    "    \n",
    "    output = Dense(1, activation='linear', name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. ENTRAÎNEMENT PAR FAMILLE (STRATÉGIE GAGNANTE)\n",
    "# ==============================================================================\n",
    "all_preds = []\n",
    "test_ids_full = []\n",
    "\n",
    "# On prépare le dataset de test global pour récupérer les IDs à la fin\n",
    "test_global = data[data['is_train'] == 0].copy()\n",
    "\n",
    "print(\"\\nDébut de l'entraînement par famille...\")\n",
    "\n",
    "for fam in FAMILIES:\n",
    "    print(f\"\\n--- Traitement de la famille : {fam} ---\")\n",
    "    \n",
    "    # Filtrer les données pour cette famille\n",
    "    df_fam = data[data['family'] == fam].copy()\n",
    "    \n",
    "    # Séparation Train/Val/Test pour cette famille\n",
    "    train_df = df_fam[df_fam['is_train'] == 1]\n",
    "    test_df = df_fam[df_fam['is_train'] == 0]\n",
    "    \n",
    "    # Validation temporelle (15 derniers jours)\n",
    "    last_date = train_df['date'].max()\n",
    "    val_start = last_date - pd.DateOffset(days=15)\n",
    "    \n",
    "    mask_train = train_df['date'] < val_start\n",
    "    mask_val = train_df['date'] >= val_start\n",
    "    \n",
    "    # Préparation inputs Keras\n",
    "    def get_inputs(df):\n",
    "        X_num = df[NUM_COLS].values.astype('float32')\n",
    "        X_cat = [df[c].values.astype('int32') for c in CAT_COLS]\n",
    "        return X_cat + [X_num] # Liste [cat1, cat2..., num]\n",
    "    \n",
    "    X_train = get_inputs(train_df[mask_train])\n",
    "    y_train = train_df.loc[mask_train, 'sales'].values.astype('float32')\n",
    "    \n",
    "    X_val = get_inputs(train_df[mask_val])\n",
    "    y_val = train_df.loc[mask_val, 'sales'].values.astype('float32')\n",
    "    \n",
    "    X_test = get_inputs(test_df)\n",
    "    ids_test = test_df['id'].values\n",
    "    \n",
    "    # Construction du modèle\n",
    "    model = build_resnet_model(len(NUM_COLS), CAT_COLS, label_encoders)\n",
    "    \n",
    "    # Callbacks\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=0)\n",
    "    \n",
    "    # Entraînement\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=20, # 20 epochs suffisent souvent par famille\n",
    "        batch_size=512, # Batch plus petit car moins de données\n",
    "        callbacks=[es, rlr],\n",
    "        verbose=0 # On réduit le bruit, affiche juste le résultat final\n",
    "    )\n",
    "    \n",
    "    val_loss = min(history.history['val_loss'])\n",
    "    print(f\"Famille {fam} -> Meilleure Val MSE: {val_loss:.4f} (RMSE: {np.sqrt(val_loss):.4f})\")\n",
    "    \n",
    "    # Prédiction\n",
    "    preds_log = model.predict(X_test, batch_size=512, verbose=0).flatten()\n",
    "    preds = np.expm1(preds_log)\n",
    "    preds[preds < 0] = 0\n",
    "    \n",
    "    # Stockage\n",
    "    df_res = pd.DataFrame({'id': ids_test, 'sales': preds})\n",
    "    all_preds.append(df_res)\n",
    "    \n",
    "    # Nettoyage mémoire\n",
    "    del model, X_train, X_val, X_test\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. ASSEMBLAGE ET SOUMISSION\n",
    "# ==============================================================================\n",
    "print(\"\\nAssemblage des prédictions...\")\n",
    "submission = pd.concat(all_preds).sort_values('id')\n",
    "submission.to_csv('submission_resnet_per_family.csv', index=False)\n",
    "print(\"Fichier 'submission_resnet_per_family.csv' généré avec succès !\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec958c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
