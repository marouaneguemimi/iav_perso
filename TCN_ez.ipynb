{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefac226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Embedding, Concatenate, Dropout, Conv1D, SpatialDropout1D, Add, Activation, GlobalAveragePooling1D, Reshape, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "import os\n",
    "\n",
    "# Configuration pour la reproductibilité\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CHARGEMENT ET PRÉPARATION DES DONNÉES\n",
    "# ==============================================================================\n",
    "print(\"Chargement des données...\")\n",
    "# Assure-toi que le fichier est bien généré par ton script précédent\n",
    "data = pd.read_csv('data/processed_data.csv')\n",
    "\n",
    "# Conversion de la date\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# --- LOG-TRANSFORMATION DE LA CIBLE ---\n",
    "# Critique pour la métrique RMSLE\n",
    "data.loc[data['is_train'] == 1, 'sales'] = np.log1p(data.loc[data['is_train'] == 1, 'sales'])\n",
    "\n",
    "# --- NETTOYAGE ---\n",
    "if 'transactions' in data.columns:\n",
    "    data.drop(columns=['transactions'], inplace=True)\n",
    "\n",
    "# Définition des types de features\n",
    "# On exclut 'id', 'sales', 'is_train', 'date'\n",
    "ALL_COLS = [c for c in data.columns if c not in ['id', 'sales', 'is_train', 'date']]\n",
    "\n",
    "# Features pour les Embeddings (Catégories à haute cardinalité ou cycliques)\n",
    "CAT_COLS = ['store_nbr', 'family', 'city', 'state', 'type', 'cluster', 'month', 'dayofweek']\n",
    "\n",
    "# Features Numériques (Tout le reste : Lags, Rolling means, Oil, etc.)\n",
    "NUM_COLS = [c for c in ALL_COLS if c not in CAT_COLS]\n",
    "\n",
    "print(f\"Nombre de features numériques pour le TCN : {len(NUM_COLS)}\")\n",
    "print(f\"Nombre de features catégorielles : {len(CAT_COLS)}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. PREPROCESSING (ENCODING & SCALING)\n",
    "# ==============================================================================\n",
    "print(\"Preprocessing...\")\n",
    "\n",
    "# A. Label Encoding pour les Embeddings\n",
    "label_encoders = {}\n",
    "for col in CAT_COLS:\n",
    "    le = LabelEncoder()\n",
    "    data[col] = data[col].astype(str) # Sécurité\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# B. Scaling des numériques (Indispensable pour les réseaux de neurones)\n",
    "# On utilise StandardScaler pour centrer autour de 0, mieux pour les TCN\n",
    "scaler = StandardScaler()\n",
    "data[NUM_COLS] = scaler.fit_transform(data[NUM_COLS])\n",
    "# Remplacer les NaN potentiels créés par le scaling (division par zero rare)\n",
    "data[NUM_COLS] = data[NUM_COLS].fillna(0)\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. SÉPARATION TRAIN / VALIDATION / TEST\n",
    "# ==============================================================================\n",
    "print(\"Séparation des jeux de données...\")\n",
    "\n",
    "train_df = data[data['is_train'] == 1].copy()\n",
    "test_df = data[data['is_train'] == 0].copy()\n",
    "\n",
    "# Validation temporelle (Les 15 derniers jours du train)\n",
    "last_date = train_df['date'].max()\n",
    "val_start = last_date - pd.DateOffset(days=15)\n",
    "\n",
    "train_mask = train_df['date'] < val_start\n",
    "val_mask = train_df['date'] >= val_start\n",
    "\n",
    "# Création des arrays numpy pour Keras\n",
    "def get_keras_data(df, num_cols, cat_cols):\n",
    "    # 1. Input Numérique : Shape (N, Features)\n",
    "    X_num = df[num_cols].values.astype('float32')\n",
    "    \n",
    "    # 2. Inputs Catégoriels : Liste de arrays (N, 1)\n",
    "    X_cat = [df[c].values.astype('int32') for c in cat_cols]\n",
    "    \n",
    "    return X_num, X_cat\n",
    "\n",
    "X_train_num, X_train_cat = get_keras_data(train_df[train_mask], NUM_COLS, CAT_COLS)\n",
    "y_train = train_df.loc[train_mask, 'sales'].values.astype('float32')\n",
    "\n",
    "X_val_num, X_val_cat = get_keras_data(train_df[val_mask], NUM_COLS, CAT_COLS)\n",
    "y_val = train_df.loc[val_mask, 'sales'].values.astype('float32')\n",
    "\n",
    "X_test_num, X_test_cat = get_keras_data(test_df, NUM_COLS, CAT_COLS)\n",
    "test_ids = test_df['id'].values\n",
    "\n",
    "# Nettoyage mémoire\n",
    "del data, train_df, test_df\n",
    "gc.collect()\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. ARCHITECTURE TCN (TEMPORAL CONVOLUTIONAL NETWORK)\n",
    "# ==============================================================================\n",
    "print(\"Construction du modèle TCN...\")\n",
    "\n",
    "def residual_block(x, filters, kernel_size, dilation_rate, dropout_rate=0.1):\n",
    "    \"\"\"Bloc résiduel TCN standard : Conv1D dilatée + Skip Connection\"\"\"\n",
    "    # Sauvegarde de l'entrée pour la connexion résiduelle\n",
    "    shortcut = x\n",
    "    \n",
    "    # 1ère Conv Dilatée\n",
    "    x = Conv1D(filters=filters, \n",
    "               kernel_size=kernel_size, \n",
    "               dilation_rate=dilation_rate, \n",
    "               padding='causal',  # Important pour ne pas voir le futur (même si ici on est sur des features)\n",
    "               activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = SpatialDropout1D(dropout_rate)(x)\n",
    "    \n",
    "    # 2ème Conv (Optionnelle, ici on simplifie avec une seule par bloc pour la vitesse)\n",
    "    \n",
    "    # Adaptation de la dimension du shortcut si nécessaire (si le nb de filtres change)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters=filters, kernel_size=1, padding='same')(shortcut)\n",
    "        \n",
    "    # Addition (Skip Connection)\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "def build_tcn_model(num_features, cat_cols_info, label_encoders):\n",
    "    inputs = []\n",
    "    embeddings = []\n",
    "    \n",
    "    # --- BRANCHE 1 : EMBEDDINGS (Catégories) ---\n",
    "    for col in cat_cols_info:\n",
    "        vocab_size = len(label_encoders[col].classes_) + 1\n",
    "        embed_dim = min(50, (vocab_size + 1) // 2) # Dimension raisonnable\n",
    "        \n",
    "        inp = Input(shape=(1,), name=f'input_{col}')\n",
    "        inputs.append(inp)\n",
    "        \n",
    "        emb = Embedding(vocab_size, embed_dim)(inp)\n",
    "        emb = Flatten()(emb)\n",
    "        embeddings.append(emb)\n",
    "    \n",
    "    # Concaténation des embeddings\n",
    "    x_cat = Concatenate()(embeddings)\n",
    "    x_cat = Dense(64, activation='relu')(x_cat) # Pré-traitement des catégories\n",
    "    \n",
    "    # --- BRANCHE 2 : TCN (Numériques) ---\n",
    "    # Input numérique standard (Batch, Num_Features)\n",
    "    input_num = Input(shape=(num_features,), name='input_numeric')\n",
    "    inputs.append(input_num)\n",
    "    \n",
    "    # TRUC : On reshape pour faire croire au TCN que c'est une séquence\n",
    "    # On transforme (Batch, Features) -> (Batch, Features, 1)\n",
    "    # Le TCN va \"scanner\" les features comme si c'était du temps\n",
    "    x_tcn = Reshape((num_features, 1))(input_num)\n",
    "    \n",
    "    # Bloc TCN \"Lite\" (Petit pour tester vite)\n",
    "    # Dilation rate augmente la fenêtre de vision : 1, 2, 4, 8...\n",
    "    x_tcn = residual_block(x_tcn, filters=32, kernel_size=3, dilation_rate=1)\n",
    "    x_tcn = residual_block(x_tcn, filters=32, kernel_size=3, dilation_rate=2)\n",
    "    x_tcn = residual_block(x_tcn, filters=32, kernel_size=3, dilation_rate=4)\n",
    "    \n",
    "    # Pooling pour récupérer les infos les plus importantes des filtres\n",
    "    x_tcn = GlobalAveragePooling1D()(x_tcn)\n",
    "    \n",
    "    # --- FUSION ---\n",
    "    x = Concatenate()([x_cat, x_tcn])\n",
    "    \n",
    "    # Tête de prédiction (Dense)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    \n",
    "    # Output (Régression linéaire)\n",
    "    output = Dense(1, activation='linear', name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Construction\n",
    "model = build_tcn_model(len(NUM_COLS), CAT_COLS, label_encoders)\n",
    "model.summary()\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. ENTRAÎNEMENT\n",
    "# ==============================================================================\n",
    "print(\"\\nDébut de l'entraînement...\")\n",
    "\n",
    "# Callbacks pour éviter le sur-apprentissage et optimiser le LR\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-5, verbose=1)\n",
    "]\n",
    "\n",
    "# Préparation des inputs sous forme de liste pour Keras [cat1, cat2, ..., num]\n",
    "train_inputs = X_train_cat + [X_train_num]\n",
    "val_inputs = X_val_cat + [X_val_num]\n",
    "\n",
    "history = model.fit(\n",
    "    train_inputs, y_train,\n",
    "    validation_data=(val_inputs, y_val),\n",
    "    epochs=15,          # Petit nombre pour tester vite (augmente à 30-50 si ça marche bien)\n",
    "    batch_size=2048,    # Gros batch pour aller vite\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. PRÉDICTION ET SOUMISSION\n",
    "# ==============================================================================\n",
    "print(\"\\nGénération des prédictions...\")\n",
    "\n",
    "test_inputs = X_test_cat + [X_test_num]\n",
    "preds_log = model.predict(test_inputs, batch_size=2048).flatten()\n",
    "\n",
    "# Inverse Log (expm1)\n",
    "preds = np.expm1(preds_log)\n",
    "preds[preds < 0] = 0 # Sécurité\n",
    "\n",
    "submission = pd.DataFrame({'id': test_ids, 'sales': preds})\n",
    "submission.to_csv('submission_tcn_lite.csv', index=False)\n",
    "\n",
    "print(\"Terminé ! Fichier 'submission_tcn_lite.csv' généré.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4a15d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
