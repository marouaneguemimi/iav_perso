{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b6233",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CHARGEMENT ET PRÉPARATION\n",
    "# ==============================================================================\n",
    "print(\"Chargement des données...\")\n",
    "# Assure-toi que le fichier est bien celui généré par ton script de processing\n",
    "data = pd.read_csv('data/processed_data.csv', parse_dates=['date'])\n",
    "\n",
    "# --- LOG-TRANSFORMATION DE LA CIBLE ---\n",
    "# On applique log1p sur les ventes d'entraînement pour la métrique RMSLE\n",
    "data.loc[data['is_train'] == 1, 'sales'] = np.log1p(data.loc[data['is_train'] == 1, 'sales'])\n",
    "\n",
    "# Suppression de la colonne transactions brute si elle existe (on utilise les lags)\n",
    "if 'transactions' in data.columns:\n",
    "    data.drop(columns=['transactions'], inplace=True)\n",
    "\n",
    "# Conversion en type 'category' pour LightGBM (Optimisation mémoire et vitesse)\n",
    "# Note : On ne met PAS 'family' ici car on va boucler dessus, elle ne sera pas une feature\n",
    "categorical_features = [\n",
    "    \"store_nbr\", \"city\", \"state\", \"type\", \"cluster\",\n",
    "    \"is_holiday\", \"dayofweek\", \"month\"\n",
    "]\n",
    "\n",
    "for col in categorical_features:\n",
    "    if col in data.columns:\n",
    "        data[col] = data[col].astype('category')\n",
    "\n",
    "# Liste des features (On exclut 'family' car c'est notre boucle, et les IDs/Dates)\n",
    "features = [col for col in data.columns if col not in ['id', 'sales', 'is_train', 'date', 'family']]\n",
    "\n",
    "print(f\"Features utilisées : {len(features)}\")\n",
    "print(features)\n",
    "\n",
    "# Liste des familles uniques\n",
    "FAMILIES = data['family'].unique()\n",
    "print(f\"Nombre de modèles à entraîner : {len(FAMILIES)}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. BOUCLE D'ENTRAÎNEMENT PAR FAMILLE\n",
    "# ==============================================================================\n",
    "all_predictions = []\n",
    "test_ids_global = []\n",
    "\n",
    "# Paramètres LightGBM (Basés sur ta baseline précédente)\n",
    "lgb_params = {\n",
    "    'objective': 'regression_l1', # Ou 'regression' (mse), mais l1 est souvent robuste\n",
    "    'metric': 'rmse',\n",
    "    'n_estimators': 1500,         # Un peu moins que le global car moins de données par famille\n",
    "    'learning_rate': 0.02,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1,\n",
    "    'lambda_l1': 0.1,\n",
    "    'lambda_l2': 0.1,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1,\n",
    "    'n_jobs': -1,\n",
    "    'seed': 42,\n",
    "    'boosting_type': 'gbdt',\n",
    "}\n",
    "\n",
    "print(\"\\nDébut de l'entraînement par famille...\")\n",
    "\n",
    "for i, fam in enumerate(FAMILIES):\n",
    "    print(f\"\\n[{i+1}/{len(FAMILIES)}] Traitement de la famille : {fam}\")\n",
    "    \n",
    "    # 1. Filtrer les données pour cette famille uniquement\n",
    "    df_fam = data[data['family'] == fam].copy()\n",
    "    \n",
    "    # 2. Séparation Train / Test\n",
    "    train_df = df_fam[df_fam['is_train'] == 1]\n",
    "    test_df = df_fam[df_fam['is_train'] == 0]\n",
    "    \n",
    "    # 3. Création du jeu de validation (15 derniers jours du train de cette famille)\n",
    "    last_train_date = train_df['date'].max()\n",
    "    validation_start_date = last_train_date - pd.DateOffset(days=15)\n",
    "    \n",
    "    valid_mask = train_df['date'] >= validation_start_date\n",
    "    train_mask = train_df['date'] < validation_start_date\n",
    "    \n",
    "    X_train = train_df.loc[train_mask, features]\n",
    "    y_train = train_df.loc[train_mask, 'sales']\n",
    "    \n",
    "    X_valid = train_df.loc[valid_mask, features]\n",
    "    y_valid = train_df.loc[valid_mask, 'sales']\n",
    "    \n",
    "    X_test = test_df[features]\n",
    "    ids_test = test_df['id'] # On garde les IDs pour la fin\n",
    "    \n",
    "    # 4. Entraînement du modèle\n",
    "    model = lgb.LGBMRegressor(**lgb_params)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        eval_metric='rmse',\n",
    "        callbacks=[\n",
    "            lgb.log_evaluation(period=0), # 0 pour ne pas spammer la console\n",
    "            lgb.early_stopping(stopping_rounds=50)\n",
    "        ],\n",
    "        categorical_feature=[c for c in categorical_features if c in features]\n",
    "    )\n",
    "    \n",
    "    # Affichage du score de validation pour cette famille\n",
    "    best_score = model.best_score_['valid_0']['rmse']\n",
    "    print(f\"   -> Best Validation RMSE: {best_score:.4f}\")\n",
    "    \n",
    "    # 5. Prédiction\n",
    "    preds_log = model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "    \n",
    "    # Inverse Log (expm1) et correction des négatifs\n",
    "    preds = np.expm1(preds_log)\n",
    "    preds[preds < 0] = 0\n",
    "    \n",
    "    # Stockage des résultats\n",
    "    fam_submission = pd.DataFrame({'id': ids_test, 'sales': preds})\n",
    "    all_predictions.append(fam_submission)\n",
    "    \n",
    "    # Nettoyage mémoire immédiat\n",
    "    del df_fam, train_df, test_df, X_train, X_valid, X_test, model\n",
    "    gc.collect()\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. ASSEMBLAGE FINAL\n",
    "# ==============================================================================\n",
    "print(\"\\nAssemblage de toutes les prédictions...\")\n",
    "\n",
    "# Concaténer tous les dataframes de chaque famille\n",
    "submission_df = pd.concat(all_predictions)\n",
    "\n",
    "# Trier par ID pour respecter l'ordre de Kaggle (Très important)\n",
    "submission_df = submission_df.sort_values('id').reset_index(drop=True)\n",
    "\n",
    "# Sauvegarde\n",
    "filename = 'submission_lgbm_per_family.csv'\n",
    "submission_df.to_csv(filename, index=False)\n",
    "\n",
    "print(f\"\\nFichier '{filename}' créé avec succès !\")\n",
    "print(submission_df.head())\n",
    "\n",
    "# Vérification rapide de la cohérence (pas de trous)\n",
    "expected_len = 28512 # Taille standard du test set Store Sales\n",
    "if len(submission_df) == expected_len:\n",
    "    print(f\"Taille du fichier correcte : {len(submission_df)} lignes.\")\n",
    "else:\n",
    "    print(f\"ATTENTION : Taille incorrecte ({len(submission_df)} au lieu de {expected_len})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955ea1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5519276b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
